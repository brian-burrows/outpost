# Outpost: A Historical Weather Tracking Application

Outpost is themed as a weather-tracking application, but the core purpose is to serve as a distributed systems benchmarking environment.
This project demonstrates my ability to design scalable, reliable, observable backend architectures in a cloud-native environment.

Outpost addresses a critical gap for outdoor enthusiasts: forecasts alone are insufficient.
Real-time conditions (like mud, high water, or snowpack stability) are heavily influenced by the preceding days of weather history.

Outpost is designed to track these historical conditions. It allows users to specify weather preferences, track them across multiple locations, view historical data maps, and receive alerts when conditions align with their ideal scenario.

---

## Proposed system architecture

Outpost is designed as a greenfield, microservices application that will run in AWS.

## The Goal: A Distributed Systems Benchmark

This repository is a working distributed systems sandbox and benchmarking environment focused on high-performance backend architecture.
The following sections describe core proficiencies and things that I'm interested in learning.

### Core Demonstrated Skills

- **Data & Reliability:** Testing various delivery semantics (e.g., At-Least-Once), guarantees, and queueing patterns for ensuring reliable, quick, and scalable data delivery.
- **Performance Optimization:** Leveraging asynchronous event loops (`asyncio`), multiprocessing, and multi-threading for maximal system throughput and minimizing I/O blocking.
- **System Resilience:** Implementing defensive patterns to protect server resources from client overload (e.g., **rate-limiting**) and optimizing system performance with **caching strategies** (e.g., Redis).
- **Orchestration & Deployment:** Configuring modern tools for CI/CD pipelines and exploring advanced deployment strategies, including **Canary rollouts**, **blue/green testing**, and automated **rollbacks**.

---

## Scope and Approach

To maintain a strict focus on the distributed systems architecture, the application's front-end and external dependencies are out of scope. Third-party API calls are **mocked**, and datasets are generated **procedurally or stochastically** to simulate realistic, large-scale data flow for benchmarking purposes.

The primary goal is demonstrating the robustness of the **backend services** and **orchestration patterns.**

---

## Key Technologies Used

- **Python**: The primary language for development, focusing on **ETL pipelines** (producers/consumers) and high-performance backend APIs using **Asynchronous FastAPI**.
- **Golang** (Future Work): Being tested as an alternative, concurrent language for serving APIs to benchmark performance and resource utilization against Python.

---

### API Engineering & Frontend Interface

- **FastAPI**: Used to build robust and highly performant **RESTful APIs**.
- **API Gateway (Nginx)**: Used to manage the edge of the service, handling **cross-cutting concerns** like centralized **rate-limiting**, request routing, and **JWT verification**.
- **GraphQL** (Future): To assess providing a more **flexible and efficient data querying API** for the customer-facing layer, allowing clients to request only the data they need.
- **OpenAPI / Swagger**: For generating **standardized API documentation** to ensure clear developer experience. Automatically generated by FastAPI.
- **RPC (gRPC)**: Exploring for **low-latency, high-throughput inter-service communication** between core microservices.
- **Pytest**: Testing framework used to implement unit and integration tests.

---

### Data Engineering & Pipelines

- **Redis**: Utilized for **distributed caching** (to improve response times), **rate limiting**, and **deduplication** of incoming requests.
- **Redis Streams**: Implemented as a **fault-tolerant streaming platform** to decouple services and handle high-throughput, **event-based data ingestion** from the simulated weather source.
- **S3**: Highly available, distributed key value store to serve as a data lakehouse.
- **PostgreSQL**: The relational data store chosen for its **robustness**, **transactional integrity**, and **JSONB support** for complex, semi-structured data storage.
- **Apache Airflow**: Future implementation to manage scheduling, monitoring, and dependency management between complex ETL jobs.
- **dbt (Data Build Tool)**: Future tool for managing and version-controlling **data transformations** within the PostgreSQL warehouse.

---

### Platform Engineering & Observability

- **Docker Compose**: For rapid, isolated environment prototyping and defining local service configurations.
- **Kubernetes (K8s)**: The production-grade solution for container orchestration, service discovery, and networking.
- **Helm**: Manage deployments, configuration, and file injection to Kubernetes manifests.
- **AWS CDK**: Planned tool for \*\*Infrastructure as Code (IaC)\*\*, automating the provisioning and management of the underlying cloud resources.
- **Kubernetes Event Driven Autoscaler (KEDA)**: To implement **cost-effective, reactive scaling** of worker nodes based on real-time queue depth metrics (e.g., Kafka/Redis queue length).
- **Prometheus** & **Grafana**: Used together for comprehensive **time-series metrics** collection and building **system-level dashboards** to monitor performance and health.
- **Jaeger / OpenTelemetry**: Implementation for **distributed tracing** to enable root-cause analysis and profile latency across service boundaries.
- **Slack**: Serves as a reliable data sink for **internal alerting** triggered by metrics thresholds.
- **GitHub**: For storing source code, managing CI/CD workflows, and planning future work (using **Kanban**).

### Data science and machine learning

- **Sklearn**: Used to generate simple classifications of weather data, along with forecasts of conditions (future).
- **Optuna**: Used to automate hyperparameter tuning pipelines.
- **Lightgbm**: Serves as a scalable classification and regression framework.

## Running the service

### Docker Compose

A Docker Compose application is defined in `./docker-compose.yaml`,
which can run a local version of the entire application using `docker compose up --build -d`

### Kubernetes (Minikube)

Minikube runs itself in Docker.
The in-cluster registry listens on the standard port `5000`.
But, Minikube needs to expose this port to your localhost,
which may have port `5000` in use by Docker.
A random, high value, port is selected, which would require reconfiguration on startup.
Therefore, you need to handle the mapping.
Running the following command will force Docker to build images in the Minikube VM

`eval $(minikube docker-env)`

That way, Kubernetes knows about the images you build, and can pull them.

Images are built using docker compose using `docker compose build`
The container names are compatible with the underlying Kubernetes Manifests.
For this to work, the image names in the manifests much match those in the `docker-compose.yaml` file.

To clean up the service, run `eval $(minikube docker-env -u)` after testing is complete.

## Useful resources

- https://microservices.io/
- https://microservices.io/patterns/data/transactional-outbox.html
