from abc import ABC, abstractmethod
from owm.tasks import TaskType
import redis
import json
from typing import Type, Any
import logging 

LOGGER = logging.getLogger(__name__)

class TaskQueueRepositoryInterface(ABC):
    @abstractmethod
    def dequeue_tasks(self, count: int, block_ms: int | None = None) -> list[tuple[str, TaskType]]:
        """Fetches new tasks from the queue"""
        pass
    
    @abstractmethod
    def enqueue_tasks(self, task: list[TaskType]) -> list[Any]:
        pass

    @abstractmethod
    def recover_stuck_tasks(self, max_idle_ms: int, max_retries: int, batch_size: int) -> tuple[
        list[tuple[str, TaskType]],
        list[tuple[str, TaskType]]
    ]:
        """Scans for and attempts to reclaim or flag stuck/failed tasks."""
        pass

    @abstractmethod
    def acknowledge_tasks(self, message_ids: list[str]) -> int:
        """Confirms processing of tasks, removing them from the pending list."""
        pass


class RedisTaskQueueRepository(TaskQueueRepositoryInterface):
    def __init__(
        self,
        client: redis.StrictRedis,
        stream_key: str,
        task_model: Type[TaskType],
        group_name: str | None = None,
        consumer_name: str | None = None,
        max_stream_length: int | None = None
    ):
        LOGGER.info(f"Connected to client {client.info}")
        self.redis_connection = client
        self.stream_key = stream_key 
        self.task_model = task_model 
        self.group_name = group_name 
        self.consumer_name = consumer_name
        self.max_stream_length = max_stream_length
        if group_name and consumer_name:
            self._ensure_consumer_group_exists()

    def _ensure_consumer_group_exists(self):
        try:
            # Create group starting at '$' (new messages only), mkstream=True creates stream if missing
            self.redis_connection.xgroup_create(name=self.stream_key, groupname=self.group_name, id='$', mkstream=True)
            LOGGER.info(f"Consumer group '{self.group_name}' created for stream '{self.stream_key}'")
        except redis.exceptions.ResponseError as e:
            if "BUSYGROUP" in str(e):
                LOGGER.debug(f"Consumer group '{self.group_name}' already exists")
            else:
                LOGGER.error(f"Error creating consumer group: {e}")
                raise

    def _deserialize_response(self, response: list[tuple[bytes, list[tuple[bytes, bytes]]]]) -> list[tuple[str, TaskType]]:
        """Deserialize a response object from Redis.

        Tasks are assumed to be produced by a producer in the following manner:
            Key: b'data'
            Value: byte-encoded JSON string using utf-8

        Where the Serialized JSON string must have been generated using `Pydantic.BaseModel.model_dumps`
        associated with the `task_type` class used to instantiate the `RedisStreamConsumer`.

        The message ID must be a Redis-generated timestamp.

        Parameters
        ----------
        response : list[tuple[bytes, list[tuple[bytes, bytes]]]
            Redis Stream responses are provided as:
                `[[stream_key, [[message_id, fields], [message_id, fields], ...]]]`
            where `message_id` and `fields` are byte-encoded strings. In this class,
            `fields` are byte-encoded JSON used to instantiate a `TaskType` instance.
            
        Returns
        -------
        List[Tuple[str, TaskType]]
            Returns a list of messages. Each message has a string-represented timestamp (message_id)
            which was auto-generated by Redis at the time the message was produced. The first
            entry in the tuple is this message ID. The second entry is a Pydantic model
            instantiated with the data contained in the message.

        """
        tasks_with_ids = []
        if response and response[0][1]:
            for message_id, fields in response[0][1]:
                task_data = json.loads(fields.get(b'data', b'{}'))
                task = self.task_model(**task_data) 
                tasks_with_ids.append((message_id.decode('utf-8'), task))
        return tasks_with_ids
    
    def _serialize_task(self, task: list[TaskType]) -> list[str, bytes]:
        return (task.task_id, task.model_dump_json())
    
    def enqueue_tasks(self, tasks : list[TaskType]) -> list[Any]:
        LOGGER.debug(f"Attempting to enqueue {len(tasks)} tasks to queue {self.stream_key}")
        tasks = list(map(self._serialize_task, tasks))
        LOGGER.debug("Successfully serialized tasks")
        submitted_task_ids = []
        pipe = self.redis_connection.pipeline()
        for task_id, task_bytes in tasks:
            pipe.xadd(
                name=self.stream_key,
                fields={"data": task_bytes},
                maxlen=self.max_stream_length
            )
            submitted_task_ids.append(task_id)
        pipe.execute()
        LOGGER.debug(f"Successfully executed pipeline to send {len(tasks)} to Redis {self.stream_key}")
        return submitted_task_ids
    
    def dequeue_tasks(self, count: int, block_ms: int | None = None) -> list[tuple[str, TaskType]]:
        """Fetches new tasks from the queue"""
        LOGGER.info(f"Attempting to fetch {count} tasks from {self.stream_key}")
        response = self.redis_connection.xreadgroup(
            groupname=self.group_name,
            consumername=self.consumer_name,
            streams={self.stream_key: '>'},
            count=count,
            block=block_ms,
        )
        tasks = self._deserialize_response(response)
        LOGGER.info(f"Found and claimed {len(tasks)} tasks from {self.stream_key}")
        return tasks

    def acknowledge_tasks(self, message_ids: list[str]) -> int:
        """Confirms processing of tasks, removing them from the pending list."""
        if message_ids:
            LOGGER.info(f"Acknowledging {len(message_ids)} messages from {self.stream_key}")
            return self.redis_connection.xack(self.stream_key, self.group_name, *message_ids)  
        return 0

    def recover_stuck_tasks(self, max_idle_ms: int, max_retries: int, batch_size: int) -> tuple[
        list[tuple[str, TaskType]], 
        list[tuple[str, TaskType]]
    ]:
        # TODO: We need some state to track pagination properly, so that batch reads are done correctly
        LOGGER.info(f"Attempting to fetch {batch_size} tasks from {self.stream_key} PEL")
        pel_entries = self.redis_connection.xpending_range(
            self.stream_key,
            self.group_name,
            min='-',
            max='+',
            count=batch_size,    
        )
        message_ids_to_dlq: list[str] = []
        message_ids_to_claim: list[str] = []
        for entry in pel_entries: 
            LOGGER.debug(f"Extracted entry from PEL: {entry}")
            message_id = entry['message_id'].decode('utf-8') 
            delivery_count = entry['times_delivered']
            idle_time_ms = entry['time_since_delivered']
            if (idle_time_ms > max_idle_ms) and (delivery_count > max_retries):
                message_ids_to_dlq.append(message_id)
            elif idle_time_ms > max_idle_ms:
                message_ids_to_claim.append(message_id)
        dlq_tasks_data = []
        if message_ids_to_dlq:
            pipe = self.redis_connection.pipeline()
            for message_id in message_ids_to_dlq:
                pipe.xrange(name=self.stream_key, min=message_id, max=message_id, count=1)
            dlq_message_batches = pipe.execute()
            dlq_messages = []
            for message_list in dlq_message_batches:
                if message_list:
                    dlq_messages.append(message_list[0])
            dlq_response_to_process = [(self.stream_key.encode(), dlq_messages)]
            dlq_tasks_data = self._deserialize_response(dlq_response_to_process)
            LOGGER.info(f"Fetched {len(dlq_tasks_data)} DLQ messages.")
        claimed_tasks_data = []
        if message_ids_to_claim:
            claimed_messages = self.redis_connection.xclaim(
                name=self.stream_key,
                groupname=self.group_name,
                consumername=self.consumer_name,
                min_idle_time=max_idle_ms,
                message_ids=message_ids_to_claim,
            )
            tasks_response_to_process = [(self.stream_key.encode(), claimed_messages)]
            claimed_tasks_data = self._deserialize_response(tasks_response_to_process)
            LOGGER.info(f"Claimed {len(claimed_tasks_data)} stuck messages.")
        return claimed_tasks_data, dlq_tasks_data
